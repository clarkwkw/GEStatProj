{
    "headline": "Brain activity decoded to play back heard words", 
    "text": "The brain&apos;s representation of speech can be read out, decoded and reconstructed to play back words that a person is hearing, reports a team of researchers from the University of California, Berkeley. Brian Pasley and his colleagues recorded neural representations of speech sounds directly from the brains of conscious neurosurgical patients and used a computational model to reconstruct them accurately enough for individual words to be recognizable. Here&apos;s my news story about it, which includes an audio file of the word reconstructions, and here&apos;s a podcast interview with Pasley and senior author Bob Knight. The study involved taking recordings from a part of the brain called Wernicke&apos;s area, at the back of the left temporal lobe, which is involved in speech comprehension. Several years ago, another research team used the same technique to probe the neural mechanisms of speech production. They recorded from another speech centre called Broca&apos;s area, which is located further forward, and revealed new details about its function. Below is what I wrote about that study at the time. Surgery on conscious patients reveals sequence and timing of language processing Thinking of and saying a word is something that most of us do effortlessly many times a day. This involves a number of steps - we must select the appropriate word, decide on the proper tense, and also pronounce it correctly. The neural computations underlying these tasks are highly complex, and whether the brain performs them all at the same time, or one after the other, has been a subject of debate. This debate has now apparently been settled, by a team of American researchers who had the rare opportunity to investigate language processing in conscious epileptic patients undergoing surgery. In a study published in the journal Science, the researchers report that the brain processes lexical, grammatical and phonological information in a well defined sequence that lasts less than half a second, and that a single language centre known as Broca&apos;s Area is involved in all these tasks.  Broca&apos;s Area is named after the French physician Pierre Paul Broca, who identified the brain region during post mortem examinations of two patients who had lost the ability to speak after suffering strokes. These patients were still able to understand the speech of others perfectly well, and the area Broca identified - which is located in the inferior frontal gyrus of the left hemisphere - was later found to control the throat and tongue muscles required for production of speech. It has therefore long been assumed to be involved solely in speech production.  It is now known that Broca&apos;s Area is also involved in other aspects of speech. But the faculty of speech cannot be studied in animals, and the resolution of techniques used to investigate the living human brain, such as functional neuroimaging, is too low to examine brain activity in any great detail. So since it was discovered, little progress has been been made towards understanding the precise role of this part of the brain in language. Neurosurgeons can probe the brain using electrodes placed onto the surface of the cerebral cortex, but the operations requiring this procedure are not performed very often.  Using a variation of the technique pioneered by Wilder Penfield in the 1930s, Ned Sahin and his colleagues implanted electrode arrays into the brains of three epileptic patients undergoing pre-surgical evaluation. During the procedure, the patients were shown words on a computer screen, and asked to either silently repeat them or perform various inflections. For example, if they saw the phrase &quot;Yesterday they ____&quot; followed by &quot;to walk&quot;, the patient would mentally utter &quot;Yesterday they walked&quot; and then press a button to indicate that they had completed the task. The electrodes were implanted in and around Broca&apos;s Area, enabling the researchers to record the neural activity associated with the language processing, at high spatial and temporal resolution. This revealed that the neural signature of the language task consisted of three distinct components, which were separated in space and time, and found consistently in all three patients. The three phases of electrical activity were recorded exclusively from Broca&apos;s Area, but were recorded at different times and in distinct subregions separated from each other by several millimeters. The first component occurred at around 200 milliseconds (ms) after presentation of each word. It was found to be larger for infrequently used words than for common ones, but not sensitive to word length, suggesting that it corresponds to word identification. Broca&apos;s Area is not associated with identifying words, but it has previously been shown to activated at this timescale in response to lexical information delivered to it from other language areas. The second component of the signature was recorded at 320 ms after word repesentation, in a region at the back of Broca&apos;s Area. Activity recorded from this region was found to be modulated during trials requiring the patients to subvocally produce the past tense forms of verbs or to convert nouns between the singular and the plural, but not during trials in which the word was just repeated. It therefore seems to be involved in processing grammatical, but not lexical, aspects of language. The final component was recorded at around 450 ms, in yet another distinct subregion. This signal was the same during trials in which the patients read the word as it was presented, or uttered a sentence containing it in the present tense (for verbs) or the singular form (for nouns). However, it differed in trials involving conversion of a verb to the past tense, or of noun to its plural. These inflexions generate outputs which sound different from the others - for the past tense, one must select an appropriate suffix, such as &quot;-ed&quot;, and decide how it is pronounced (for example, the &quot;d&quot; in &quot;handed&quot; and &quot;walked&quot; sound different from one another), as well as whether it is regular (&quot;played&quot;) or irregular (&quot;bought&quot;). These differences led the researchers to the conclusion that this third component corresponds to the processing of phonological information.  According to the classical neurological model of language, Broca&apos;s Area is involved in speech production, and Wernicke&apos;s Area, which is located in the temporal lobe, is required for speech comprehension. This study shows that Broca&apos;s Area is subdivided into functionally distinct regions which are involved in sequential processing of different aspects of language, and that its role in language is far more extensive than previously thought. It adds to earlier evidence that Broca&apos;s Area is involved in both speech production and comprehension. Future work using these techniques may reveal more of its fine-grained structure, and provide further clues about its involvement in speech. Reference: Sahin, N., et al. (2009). Sequential Processing of Lexical, Grammatical, and Phonological Information Within Broca&apos;s Area. Science 326: 445-449. DOI: 10.1126/science.1174481. Originally posted at ScienceBlogs.com on October 16th, 2009.", 
    "section": "science", 
    "gid": "science/neurophilosophy/2012/jan/31/eavesdropping-brains-speech-mechanisms", 
    "wordcount": "1154"
}