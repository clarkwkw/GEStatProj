{
    "headline": "The folly of science on a shoestring", 
    "text": "Picture a talented scientist working on a shoestring budget and you may conjure up romantic ideas of a creative genius alone in a lab, struggling against the odds, but employing ingenuity to crack problems that others can only tackle using brute force (ie generous funding). In reality, there is nothing romantic about underfunded science. Consider the ill-fated British Mars lander, Beagle 2, which vanished without trace in 2003. Recently the spotlight has been turned on under-resourced experiments in neuroscience. According to new research published in a leading review journal, many neuroscience experiments lack sufficient statistical power. Put plainly, this means they do not collect enough data to achieve their scientific aims. The most obvious implication is that many neuroscientific experiments simply fail, like Beagle 2. But outright failure is not the worst thing that can happen. As Kate Button explained on this blog last week, the more serious problem with under-resourced neuroscience is that it systematically distorts the established body of supposed knowledge by increasing the number of spurious findings  so-called &quot;false positives&quot;. Exactly the same argument applies to any empirical science. There are many reasons why underpowered experiments increase the number of false positives, but perhaps a simple analogy will suffice. Imagine a photographer economises on the length of exposure of a photograph despite poor lighting conditions. The result will be a murky, underexposed image. Not only is the true picture difficult to decipher, but if you look closely enough, for long enough, you might even start to see phantoms and apparitions emerging from the shadows. Let&apos;s call these &quot;false positive&quot; findings. In empirical science, insufficient data can only develop into a murky, underexposed picture, which inevitably leads researchers to read too much into random patterns. Moreover, these spurious results are extremely difficult to dispel once they are accepted into the cannon of established fact. For this reason, false positive results are more harmful than no result at all. Insufficient data inevitably comes down to a question of funding. Scientists love data, and they especially love lots of data. But funding is always limited and expectations are always high - there are strong incentives to publish more for less. But if we are really serious about trying to understand the mysteries of the brain, then we need to invest serious money to address specific questions. Earlier this month US President Barack Obama committed $100m to launch the Brain Research through Advancing Innovative Neurotechnologies (Brain) initiative, with the ultimate aim of mapping every connection in the human brain, as scientists under previous administrations have mapped the human genome and sent astronauts to the moon. Earlier this year the European Commission also awarded 1bn to a large-scale Human Brain Project. But increasing overall funds is only part of the solution. Perhaps more importantly, we also need to dispel the myth that science on a shoestring budget is cost-effective. The metric of efficiency is difficult to quantify in most empirical sciences, but is often estimated by the ratio of research output (eg published scientific articles) to funding input. This assumes that each output reflects a worthy scientific result, but at least one influential study discovered that almost 90% of results published in preclinical cancer research were effectively false positives. So just maximising research output quantity may be anything but efficient. A proliferation of spurious results seriously adds confusion, and in many cases, leads further research down expensive and time-consuming blind alleys. So yes, science needs more funding, but we also need to rethink how the available funds are allocated. If there is a genuine commitment to funding a particular experiment, then it is essential that enough money is allocated for that experiment to be carried out properly. Funding bodies are ultimately responsible for ensuring that sufficient resources will be allocated to prospective projects, but it is crucial that scientific journals also play their part. While it remains possible to publish unreliable results from under-powered experiments, there remains an incentive to spread resources too thinly. In the wake of high profile studies that reveal the true incidence of false positive findings, scientific journals  such as Nature Neuroscience  are now taking steps to improve the screening process in an attempt to ensure the reliability of the results they publish, so there is certainly reason for optimism. We also need to reduce unrealistic demands to get more and more out of less and less, but again there is reason for optimism. The research excellence framework for assessing the quality of research at higher education institutions in the UK will only consider a handful of a scientist&apos;s best articles in a clear commitment to reward quality over quantity, helping to reduce the career incentive for spreading resources too thinly. Newton famously wrote: &quot;If I have seen further it is by standing on the shoulders of giants.&quot; We need to make sure that science continues to stand on solid foundations, not a house of cards. Mark Stokes is a senior researcher at the Oxford Centre for Human Brain Activity, University of Oxford. He has written more about the importance of sufficient data collection on his blog, Brain Box", 
    "section": "science", 
    "gid": "science/blog/2013/apr/16/folly-science-shoestring", 
    "wordcount": "857"
}